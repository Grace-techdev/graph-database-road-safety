{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b092a0e2",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Graph Database Construction\n",
    "\n",
    "This notebook implements the **ETL process** for preparing a property graph database\n",
    "for Australian road safety and fatal crash analysis.\n",
    "\n",
    "It transforms the original relational-style dataset into **node and relationship CSV files**\n",
    "compatible with **Neo4j bulk import**, following the graph schema defined in the project report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d4aa05",
   "metadata": {},
   "source": [
    "## Purpose of This Notebook\n",
    "\n",
    "The ETL pipeline performs the following tasks:\n",
    "\n",
    "- Cleans and normalizes the original crash dataset\n",
    "- Constructs node tables (Crash, Person, Vehicle, Location, Road, Time)\n",
    "- Constructs relationship tables linking crashes to related entities\n",
    "- Generates stable IDs for nodes and relationships\n",
    "- Outputs CSV files for direct Neo4j import\n",
    "\n",
    "The resulting files are stored in the `data/csv_full_datasets/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3b504-d59b-41a3-b883-5113c6684aab",
   "metadata": {},
   "source": [
    "## Academic Context\n",
    "\n",
    "This work was originally developed as **Project 2** for  \n",
    "**CITS5504 – Data Warehousing**, University of Western Australia.\n",
    "\n",
    "Author: Boya Zhang  \n",
    "Date: May 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615f0d0-fadb-4367-8705-69315fca9e87",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26f8b2-e195-4720-ab37-a5e2414550fd",
   "metadata": {},
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b83b3fd2-a15b-44bb-b24e-1781d1ceb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6b211-dd68-480f-8cb7-0bf9a2d9c569",
   "metadata": {},
   "source": [
    "**Loading the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04a7cdb4-2564-49b1-a11d-bced507f953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Crash ID</th>\n",
       "      <th>State</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Dayweek</th>\n",
       "      <th>Time</th>\n",
       "      <th>Crash Type</th>\n",
       "      <th>Number Fatalities</th>\n",
       "      <th>Bus Involvement</th>\n",
       "      <th>...</th>\n",
       "      <th>Age</th>\n",
       "      <th>National Remoteness Areas</th>\n",
       "      <th>SA4 Name 2021</th>\n",
       "      <th>National LGA Name 2024</th>\n",
       "      <th>National Road Type</th>\n",
       "      <th>Christmas Period</th>\n",
       "      <th>Easter Period</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Day of week</th>\n",
       "      <th>Time of day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20241115</td>\n",
       "      <td>NSW</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>4:00</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "      <td>Riverina</td>\n",
       "      <td>Wagga Wagga</td>\n",
       "      <td>Arterial Road</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>65_to_74</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20241125</td>\n",
       "      <td>NSW</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>6:15</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "      <td>Sydney - Baulkham Hills and Hawkesbury</td>\n",
       "      <td>Hawkesbury</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>17_to_25</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20246013</td>\n",
       "      <td>TAS</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>9:43</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "      <td>Launceston and North East</td>\n",
       "      <td>Northern Midlands</td>\n",
       "      <td>Local Road</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>26_to_39</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20241002</td>\n",
       "      <td>NSW</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>10:35</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>Outer Regional Australia</td>\n",
       "      <td>New England and North West</td>\n",
       "      <td>Armidale</td>\n",
       "      <td>National or State Highway</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>26_to_39</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20243185</td>\n",
       "      <td>QLD</td>\n",
       "      <td>12</td>\n",
       "      <td>2024</td>\n",
       "      <td>Friday</td>\n",
       "      <td>13:00</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>61</td>\n",
       "      <td>Inner Regional Australia</td>\n",
       "      <td>Toowoomba</td>\n",
       "      <td>Lockyer Valley</td>\n",
       "      <td>National or State Highway</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>40_to_64</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Crash ID State  Month  Year Dayweek   Time Crash Type  \\\n",
       "0   1  20241115   NSW     12  2024  Friday   4:00     Single   \n",
       "1   2  20241125   NSW     12  2024  Friday   6:15     Single   \n",
       "2   3  20246013   TAS     12  2024  Friday   9:43     Single   \n",
       "3   4  20241002   NSW     12  2024  Friday  10:35     Single   \n",
       "4   5  20243185   QLD     12  2024  Friday  13:00     Single   \n",
       "\n",
       "   Number Fatalities Bus Involvement  ... Age National Remoteness Areas  \\\n",
       "0                  1              No  ...  74  Inner Regional Australia   \n",
       "1                  1              No  ...  19  Inner Regional Australia   \n",
       "2                  1              No  ...  33  Inner Regional Australia   \n",
       "3                  1              No  ...  32  Outer Regional Australia   \n",
       "4                  1              No  ...  61  Inner Regional Australia   \n",
       "\n",
       "                            SA4 Name 2021 National LGA Name 2024  \\\n",
       "0                                Riverina            Wagga Wagga   \n",
       "1  Sydney - Baulkham Hills and Hawkesbury             Hawkesbury   \n",
       "2               Launceston and North East      Northern Midlands   \n",
       "3              New England and North West               Armidale   \n",
       "4                               Toowoomba         Lockyer Valley   \n",
       "\n",
       "          National Road Type  Christmas Period Easter Period Age Group  \\\n",
       "0              Arterial Road               Yes            No  65_to_74   \n",
       "1                 Local Road                No            No  17_to_25   \n",
       "2                 Local Road               Yes            No  26_to_39   \n",
       "3  National or State Highway                No            No  26_to_39   \n",
       "4  National or State Highway                No            No  40_to_64   \n",
       "\n",
       "  Day of week Time of day  \n",
       "0     Weekday       Night  \n",
       "1     Weekday         Day  \n",
       "2     Weekday         Day  \n",
       "3     Weekday         Day  \n",
       "4     Weekday         Day  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "crash_df = pd.read_csv('Project2_Dataset_Corrected.csv')\n",
    "crash_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164e8688-a315-4ee0-aed1-4edf60630ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10490, 25)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {crash_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47793b-28f5-4668-a774-4a10c573fcb0",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa48fa-b8f5-41ee-a638-79b912e37105",
   "metadata": {},
   "source": [
    "In this section, I examine the original dataset to understand its structure and assess data quality. This includes checking the column names, data types, and the presence of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29df1fa8-fcdf-415d-8373-f7c206e3badd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Crash ID', 'State', 'Month', 'Year', 'Dayweek', 'Time',\n",
       "       'Crash Type', 'Number Fatalities', 'Bus Involvement',\n",
       "       'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement',\n",
       "       'Speed Limit', 'Road User', 'Gender', 'Age',\n",
       "       'National Remoteness Areas', 'SA4 Name 2021', 'National LGA Name 2024',\n",
       "       'National Road Type', 'Christmas Period', 'Easter Period', 'Age Group',\n",
       "       'Day of week', 'Time of day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33988c94-0857-42d7-ac4e-dfb8bb228866",
   "metadata": {},
   "source": [
    "Check for Missing Values and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e553befe-ad7c-47a2-a169-c5def88fa141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10490 entries, 0 to 10489\n",
      "Data columns (total 25 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   ID                             10490 non-null  int64 \n",
      " 1   Crash ID                       10490 non-null  int64 \n",
      " 2   State                          10490 non-null  object\n",
      " 3   Month                          10490 non-null  int64 \n",
      " 4   Year                           10490 non-null  int64 \n",
      " 5   Dayweek                        10490 non-null  object\n",
      " 6   Time                           10490 non-null  object\n",
      " 7   Crash Type                     10490 non-null  object\n",
      " 8   Number Fatalities              10490 non-null  int64 \n",
      " 9   Bus Involvement                10490 non-null  object\n",
      " 10  Heavy Rigid Truck Involvement  10490 non-null  object\n",
      " 11  Articulated Truck Involvement  10490 non-null  object\n",
      " 12  Speed Limit                    10490 non-null  int64 \n",
      " 13  Road User                      10490 non-null  object\n",
      " 14  Gender                         10490 non-null  object\n",
      " 15  Age                            10490 non-null  int64 \n",
      " 16  National Remoteness Areas      10490 non-null  object\n",
      " 17  SA4 Name 2021                  10490 non-null  object\n",
      " 18  National LGA Name 2024         10490 non-null  object\n",
      " 19  National Road Type             10490 non-null  object\n",
      " 20  Christmas Period               10490 non-null  object\n",
      " 21  Easter Period                  10490 non-null  object\n",
      " 22  Age Group                      10490 non-null  object\n",
      " 23  Day of week                    10490 non-null  object\n",
      " 24  Time of day                    10490 non-null  object\n",
      "dtypes: int64(7), object(18)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "crash_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb4e18-409d-48fb-9ad7-0ab220e72b94",
   "metadata": {},
   "source": [
    "All columns contain 10,490 non-null values, meaning there is no missing data in the dataset.\n",
    "The data types are appropriate for transformation: numeric columns use int64, and categorical fields use object. Therefore, no imputation or row removal is needed.\n",
    "\n",
    "These columns will later be mapped to graph nodes, properties, and relationships based on the data model defined in Section 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e82150-7f96-4913-9d4a-52428d467dd5",
   "metadata": {},
   "source": [
    "## 3. Create Node CSVs\n",
    "\n",
    "In this section, I extract and transform the data into separate node and relationship files for Neo4j.\n",
    "\n",
    "For each node:\n",
    "1. Select the columns relevant to that entity.\n",
    "2. Drop duplicate rows to ensure each node is unique.\n",
    "3. Create a unique identifier column (ID) if the dataset does not already contain one.\n",
    "\n",
    "These steps ensure that each node file has one row per entity and can be linked via relationships in Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8511e7-4272-41ea-8197-1c6796370d9d",
   "metadata": {},
   "source": [
    "### Crash Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92518ed6-f8cb-4cd6-bbfe-1f73dc663c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crash nodes: (9683, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create Crash node file\n",
    "# Extract relevant columns and remove duplicate crash records\n",
    "crash_node_table = crash_df[['Crash ID', 'Crash Type', 'Number Fatalities']].drop_duplicates()\n",
    "\n",
    "# Export the result to a CSV file\n",
    "crash_node_table.to_csv('csv_full_datasets/crash_node.csv', index=False)\n",
    "\n",
    "# Print the number of unique crash nodes\n",
    "print(f\"Crash nodes: {crash_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bb087-fd32-4a89-b125-417eacedc60c",
   "metadata": {},
   "source": [
    "### Vehicle Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbab39eb-9f1e-4a15-979e-2952b9770469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vehicle nodes: (7, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create Vehicle node file\n",
    "# Extract vehicle-related fields and drop duplicate combinations\n",
    "vehicle_node_table = crash_df[['Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement']].drop_duplicates()\n",
    "\n",
    "# Add a unique vehicleID for each row (needed as Neo4j node ID)\n",
    "vehicle_node_table.insert(0, 'vehicleID', range(1, 1 + len(vehicle_node_table)))\n",
    "\n",
    "# Export to CSV\n",
    "vehicle_node_table.to_csv('csv_full_datasets/vehicle_node.csv', index=False)\n",
    "\n",
    "# Print the number of unique vehicle nodes\n",
    "print(f\"Vehicle nodes: {vehicle_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba309c5-2778-4f0c-9f9f-441a78b5bfd9",
   "metadata": {},
   "source": [
    "### Killed_Person Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11637b25-128f-4ed6-ae27-f0089274cef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killed_Person nodes: (10490, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create Killed_Person node file\n",
    "# Extract individual-level fields related to the person killed in the crash\n",
    "killed_person_node_table = crash_df[['ID', 'Road User', 'Gender', 'Age', 'Age Group']]\n",
    "\n",
    "# Rename ID column to personID for clarity in Neo4j\n",
    "killed_person_node_table = killed_person_node_table.rename(columns={'ID': 'personID'})\n",
    "\n",
    "# Export to CSV\n",
    "killed_person_node_table.to_csv('csv_full_datasets/killed_person_node.csv', index=False)\n",
    "\n",
    "# Print the number of person records\n",
    "print(f\"Killed_Person nodes: {killed_person_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bad87-418e-4e84-a04e-61a2353c079d",
   "metadata": {},
   "source": [
    "### Time_Info Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c513c6a-c7c4-4aaf-8964-ac8e0de6bf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeInfo nodes: (9424, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create TimeInfo node file\n",
    "# Standardise the 'Time' column to HH:mm format (e.g., 4:00 → 04:00)\n",
    "crash_df['Time'] = crash_df['Time'].apply(\n",
    "    lambda t: f\"{t.split(':')[0].zfill(2)}:{t.split(':')[1].zfill(2)}\" if pd.notnull(t) and ':' in t else t\n",
    ")\n",
    "\n",
    "# Extract all time-related fields and remove duplicates\n",
    "timeinfo_node_table = crash_df[['Time', 'Time of day', 'Day of week', 'Month', 'Year', 'Dayweek', \n",
    "                               'Christmas Period', 'Easter Period']].drop_duplicates()\n",
    "\n",
    "# Add unique ID for time info\n",
    "timeinfo_node_table.insert(0, 'timeID', range(1, 1 + len(timeinfo_node_table)))\n",
    "\n",
    "# Export to CSV\n",
    "timeinfo_node_table.to_csv('csv_full_datasets/timeinfo_node.csv', index=False)\n",
    "\n",
    "# Print the number of time nodes\n",
    "print(f\"TimeInfo nodes: {timeinfo_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c8004-2fbe-4cff-a39d-e7044fff4beb",
   "metadata": {},
   "source": [
    "## Road Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b507c642-cf6a-4c28-8e46-8eb7752ec9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road nodes: (80, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create Road node file\n",
    "# Extract road-related attributes and remove duplicates\n",
    "road_node_table = crash_df[['Speed Limit', 'National Road Type']].drop_duplicates()\n",
    "\n",
    "# Add road ID\n",
    "road_node_table.insert(0, 'roadID', range(1, 1 + len(road_node_table)))\n",
    "\n",
    "# Export to CSV\n",
    "road_node_table.to_csv('csv_full_datasets/road_node.csv', index=False)\n",
    "\n",
    "# Print number of road nodes\n",
    "print(f\"Road nodes: {road_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1a02e-1e9c-430f-a733-cea78f24d529",
   "metadata": {},
   "source": [
    "### Location Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628b001e-452e-4e26-a823-c6a1408c5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location nodes: (786, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create Location node file\n",
    "# Extract geographic fields and remove duplicates\n",
    "location_node_table = crash_df[['National LGA Name 2024', 'SA4 Name 2021', 'State', \n",
    "                               'National Remoteness Areas']].drop_duplicates()\n",
    "\n",
    "# Standardize column names for consistency\n",
    "location_node_table = location_node_table.rename(columns={\n",
    "    'National LGA Name 2024': 'lgaName',\n",
    "    'SA4 Name 2021': 'sa4',\n",
    "    'State': 'state',\n",
    "    'National Remoteness Areas': 'nationalRemotenessAreas'\n",
    "})\n",
    "\n",
    "# Add a unique location ID\n",
    "location_node_table.insert(0, 'locationID', range(1, 1 + len(location_node_table)))\n",
    "\n",
    "# Export to CSV\n",
    "location_node_table.to_csv('csv_full_datasets/location_node.csv', index=False)\n",
    "print(f\"Location nodes: {location_node_table.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811085c-76fa-4470-8b4b-a4b464666685",
   "metadata": {},
   "source": [
    "## 4. Create Relationship CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f84b46-f581-476c-be83-940ebd14dd3d",
   "metadata": {},
   "source": [
    "### Relationship 1: Vehicle -[INVOLVED_IN]-> Crash\n",
    "This relationship connects each crash to the type of vehicle involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506968a1-34b1-4434-95ba-9681f2d30fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOLVED_IN relationships: 9683\n"
     ]
    }
   ],
   "source": [
    "# Create INVOLVED_IN relationship file between Vehicle and Crash\n",
    "# Extract crash-vehicle combinations and remove duplicates\n",
    "rel_involved_in_table = crash_df[['Crash ID', 'Bus Involvement', 'Heavy Rigid Truck Involvement', \n",
    "                                'Articulated Truck Involvement']].drop_duplicates()\n",
    "\n",
    "# Merge to get vehicleID from vehicle_node_table\n",
    "rel_involved_in_table = rel_involved_in_table.merge(\n",
    "    vehicle_node_table,\n",
    "    on=['Bus Involvement', 'Heavy Rigid Truck Involvement', 'Articulated Truck Involvement'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Keep only IDs needed for relationship file\n",
    "rel_involved_in_table = rel_involved_in_table[['vehicleID', 'Crash ID']].rename(columns={'Crash ID': 'crashID'})\n",
    "\n",
    "# Export to CSV\n",
    "rel_involved_in_table.to_csv('csv_full_datasets/rel_involved_in.csv', index=False)\n",
    "\n",
    "# Show total number of relationships\n",
    "print(f\"INVOLVED_IN relationships: {len(rel_involved_in_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25614bdf-77fa-4f0e-b290-ce1c959a8545",
   "metadata": {},
   "source": [
    "### Relationship 2: KILLED_IN – Killed_Person → Crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e616a791-f4d2-4e17-9921-de55fd32f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KILLED_IN relationships: 10490\n"
     ]
    }
   ],
   "source": [
    "# Create KILLED_IN relationship between Killed_Person and Crash\n",
    "# Each person is linked to exactly one crash\n",
    "rel_killed_in_table = crash_df[['ID', 'Crash ID']].rename(columns={'ID': 'personID', 'Crash ID': 'crashID'})\n",
    "\n",
    "# Export to CSV\n",
    "rel_killed_in_table.to_csv('csv_full_datasets/rel_killed_in.csv', index=False)\n",
    "print(f\"KILLED_IN relationships: {len(rel_killed_in_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8608e-624a-4df7-8bec-109ce0ead413",
   "metadata": {},
   "source": [
    "### Relationship 3: HAPPENED_DURING – Crash → TimeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdff64c8-6793-417e-b14f-b489249187e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPENED_DURING relationships: 9683\n"
     ]
    }
   ],
   "source": [
    "# Create HAPPENED_DURING relationship between Crash and TimeInfo\n",
    "# Join based on all time-related fields to get corresponding timeID\n",
    "rel_happened_during_table = crash_df[['Crash ID', 'Time', 'Time of day', 'Day of week', 'Month', 'Year', \n",
    "                               'Dayweek', 'Christmas Period', 'Easter Period']].drop_duplicates()\n",
    "\n",
    "rel_happened_during_table = rel_happened_during_table.merge(\n",
    "    timeinfo_node_table,\n",
    "    on=['Time', 'Time of day', 'Day of week', 'Month', 'Year', 'Dayweek', \n",
    "        'Christmas Period', 'Easter Period'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "rel_happened_during_table = rel_happened_during_table[['Crash ID', 'timeID']].rename(columns={'Crash ID': 'crashID'})\n",
    "rel_happened_during_table.to_csv('csv_full_datasets/rel_happened_during.csv', index=False)\n",
    "print(f\"HAPPENED_DURING relationships: {len(rel_happened_during_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976664d-98ee-48d5-8f89-efa8d6754937",
   "metadata": {},
   "source": [
    "### Relationship 4: HAPPENED_ON – Crash → Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064e74f4-6911-4395-9736-46e40369df95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPENED_ON relationships: 9683\n"
     ]
    }
   ],
   "source": [
    "# Create HAPPENED_ON relationship between Crash and Road\n",
    "# Join crash info with road node info to get roadID\n",
    "rel_happened_on_table = crash_df[['Crash ID', 'Speed Limit', 'National Road Type']].drop_duplicates()\n",
    "\n",
    "rel_happened_on_table = rel_happened_on_table.merge(\n",
    "    road_node_table,\n",
    "    on=['Speed Limit', 'National Road Type'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "rel_happened_on_table = rel_happened_on_table[['Crash ID', 'roadID']].rename(columns={'Crash ID': 'crashID'})\n",
    "rel_happened_on_table.to_csv('csv_full_datasets/rel_happened_on.csv', index=False)\n",
    "print(f\"HAPPENED_ON relationships: {len(rel_happened_on_table)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1098bb-b352-45b9-8033-1b149d52c20f",
   "metadata": {},
   "source": [
    "### Relationship 5: LOCATED_IN – Crash → Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ad3a49-7f91-4f35-b6e0-8b877a0e99f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCATED_IN relationships: 9683\n"
     ]
    }
   ],
   "source": [
    "# Create LOCATED_IN relationship between Crash and Location\n",
    "# Join crash location info with location node to get locationID\n",
    "rel_located_in_table = crash_df[['Crash ID', 'National LGA Name 2024', 'SA4 Name 2021', 'State', \n",
    "                               'National Remoteness Areas']].drop_duplicates()\n",
    "\n",
    "rel_located_in_table = rel_located_in_table.merge(\n",
    "    location_node_table,\n",
    "    left_on=['National LGA Name 2024', 'SA4 Name 2021', 'State', 'National Remoteness Areas'],\n",
    "    right_on=['lgaName', 'sa4', 'state', 'nationalRemotenessAreas'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "rel_located_in_table = rel_located_in_table[['Crash ID', 'locationID']].rename(columns={'Crash ID': 'crashID'})\n",
    "rel_located_in_table.to_csv('csv_full_datasets/rel_located_in.csv', index=False)\n",
    "print(f\"LOCATED_IN relationships: {len(rel_located_in_table)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
